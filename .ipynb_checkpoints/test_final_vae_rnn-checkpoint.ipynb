{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b08e0413",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import data_reading, music_generation\n",
    "from models import music_generation_VAE\n",
    "from dataset import iterable_dataset\n",
    "import os\n",
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from models import feature_prediction_gru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65c07456",
   "metadata": {},
   "outputs": [],
   "source": [
    "GUITAR_VAE= \"./saved_model_and_data/vae/vae_guitar/G_vae_trained_r12s48d26e20\"\n",
    "PIANO_VAE = \"./saved_model_and_data/vae/vae_piano/P_vae_trained_r12s48d25e20\"\n",
    "DRUMS_VAE = \"./saved_model_and_data/vae/vae_drums/D_vae_trained_r12s48d25e20\"\n",
    "BASS_VAE = \"./saved_model_and_data/vae/vae_bass/B_vae_trained_r12s48d26e20\"\n",
    "STRINGS_VAE =\"./saved_model_and_data/vae/vae_strings/S_vae_trained_r12s48d26e20\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4cc8f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "GUITAR_LEARNER = \"./saved_model_and_data/learner/guitar_based_on_piano/guitar_learner_epoch80\"\n",
    "PIANO_PREDICTOR = \"./saved_model_and_data/learner/piano_next/latent_learner_gru_piano_100\"\n",
    "STRINGS_LEARNER = \"./saved_model_and_data/learner/strings_based_on_piano/string_learner_epoch60\"\n",
    "DRUMS_LEARNER = \"./saved_model_and_data/learner/drums_based_on_piano/drums_learner_epoch80\"\n",
    "BASS_LEARNER = \"./saved_model_and_data/learner/bass_based_on_piano/bass_learner_epoch80\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97a10eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "LEARNER_HIDDEN_SIZE = 512\n",
    "VAE_HIDDEN_SIZE = 512\n",
    "N_LAYERS = 3\n",
    "BATCH_SIZE=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "34383468",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvVAE(\n",
       "  (enc_conv1): Conv2d(1, 64, kernel_size=(4, 4), stride=(2, 2))\n",
       "  (enc_conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2))\n",
       "  (enc_conv3): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2))\n",
       "  (enc_conv4): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2))\n",
       "  (trans_enc_lin1): Linear(in_features=7168, out_features=512, bias=True)\n",
       "  (trans_enc_lin2): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (trans_enc_lin3): Linear(in_features=256, out_features=513, bias=True)\n",
       "  (trans_dec1): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (trans_dec2): Linear(in_features=256, out_features=512, bias=True)\n",
       "  (trans_dec3): Linear(in_features=512, out_features=7168, bias=True)\n",
       "  (dec_conv1): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(2, 2))\n",
       "  (dec_conv2): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2))\n",
       "  (dec_conv3): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2))\n",
       "  (dec_conv4): ConvTranspose2d(64, 1, kernel_size=(4, 4), stride=(2, 2))\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (unflatten): Unflatten(dim=1, unflattened_size=(512, 2, 7))\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (batch_norm64): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (batch_norm128): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (batch_norm256): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (batch_norm512): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (batch_norm_enc1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (batch_norm_enc2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (batch_norm_dec1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (batch_norm_dec2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "guitar_learner = feature_prediction_gru.FeaturePredictionGRU([\"Guitar\"], VAE_HIDDEN_SIZE+1, \n",
    "                                                             LEARNER_HIDDEN_SIZE, BATCH_SIZE, N_LAYERS, DEVICE).to(DEVICE)\n",
    "guitar_vae = music_generation_VAE.ConvVAE([\"Guitar\"], VAE_HIDDEN_SIZE)\n",
    "guitar_learner.load_state_dict(torch.load(GUITAR_LEARNER))\n",
    "guitar_learner.to(DEVICE)\n",
    "guitar_vae.load_state_dict(torch.load(GUITAR_VAE))\n",
    "guitar_vae.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "670a50ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvVAE(\n",
       "  (enc_conv1): Conv2d(1, 64, kernel_size=(4, 4), stride=(2, 2))\n",
       "  (enc_conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2))\n",
       "  (enc_conv3): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2))\n",
       "  (enc_conv4): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2))\n",
       "  (trans_enc_lin1): Linear(in_features=7168, out_features=512, bias=True)\n",
       "  (trans_enc_lin2): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (trans_enc_lin3): Linear(in_features=256, out_features=513, bias=True)\n",
       "  (trans_dec1): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (trans_dec2): Linear(in_features=256, out_features=512, bias=True)\n",
       "  (trans_dec3): Linear(in_features=512, out_features=7168, bias=True)\n",
       "  (dec_conv1): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(2, 2))\n",
       "  (dec_conv2): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2))\n",
       "  (dec_conv3): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2))\n",
       "  (dec_conv4): ConvTranspose2d(64, 1, kernel_size=(4, 4), stride=(2, 2))\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (unflatten): Unflatten(dim=1, unflattened_size=(512, 2, 7))\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (batch_norm64): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (batch_norm128): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (batch_norm256): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (batch_norm512): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (batch_norm_enc1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (batch_norm_enc2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (batch_norm_dec1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (batch_norm_dec2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "piano_predictor = feature_prediction_gru.FeaturePredictionGRU([\"Piano\"], VAE_HIDDEN_SIZE+1, \n",
    "                                                             LEARNER_HIDDEN_SIZE, BATCH_SIZE, N_LAYERS, DEVICE).to(DEVICE)\n",
    "piano_vae = music_generation_VAE.ConvVAE([\"Piano\"], VAE_HIDDEN_SIZE)\n",
    "piano_predictor.load_state_dict(torch.load(PIANO_PREDICTOR))\n",
    "piano_predictor.to(DEVICE)\n",
    "piano_vae.load_state_dict(torch.load(PIANO_VAE))\n",
    "piano_vae.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7892019d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvVAE(\n",
       "  (enc_conv1): Conv2d(1, 64, kernel_size=(4, 4), stride=(2, 2))\n",
       "  (enc_conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2))\n",
       "  (enc_conv3): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2))\n",
       "  (enc_conv4): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2))\n",
       "  (trans_enc_lin1): Linear(in_features=7168, out_features=512, bias=True)\n",
       "  (trans_enc_lin2): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (trans_enc_lin3): Linear(in_features=256, out_features=513, bias=True)\n",
       "  (trans_dec1): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (trans_dec2): Linear(in_features=256, out_features=512, bias=True)\n",
       "  (trans_dec3): Linear(in_features=512, out_features=7168, bias=True)\n",
       "  (dec_conv1): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(2, 2))\n",
       "  (dec_conv2): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2))\n",
       "  (dec_conv3): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2))\n",
       "  (dec_conv4): ConvTranspose2d(64, 1, kernel_size=(4, 4), stride=(2, 2))\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (unflatten): Unflatten(dim=1, unflattened_size=(512, 2, 7))\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (batch_norm64): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (batch_norm128): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (batch_norm256): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (batch_norm512): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (batch_norm_enc1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (batch_norm_enc2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (batch_norm_dec1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (batch_norm_dec2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strings_learner = feature_prediction_gru.FeaturePredictionGRU([\"Strings\"], VAE_HIDDEN_SIZE+1, \n",
    "                                                             LEARNER_HIDDEN_SIZE, BATCH_SIZE, N_LAYERS, DEVICE).to(DEVICE)\n",
    "strings_vae = music_generation_VAE.ConvVAE([\"Strings\"], VAE_HIDDEN_SIZE)\n",
    "strings_learner.load_state_dict(torch.load(STRINGS_LEARNER))\n",
    "strings_learner.to(DEVICE)\n",
    "strings_vae.load_state_dict(torch.load(STRINGS_VAE))\n",
    "strings_vae.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "551c7e9b",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './saved_model_and_data/vae/vae_bass/B_vae_trained_r12s48d25e20'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_21720/3349586619.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mbass_learner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBASS_LEARNER\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mbass_learner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mbass_vae\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBASS_VAE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mbass_vae\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m    697\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'encoding'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 699\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    700\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m             \u001b[1;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    229\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    230\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 231\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    232\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;34m'w'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 212\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    213\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './saved_model_and_data/vae/vae_bass/B_vae_trained_r12s48d25e20'"
     ]
    }
   ],
   "source": [
    "bass_learner = feature_prediction_gru.FeaturePredictionGRU([\"Bass\"], VAE_HIDDEN_SIZE+1, \n",
    "                                                             LEARNER_HIDDEN_SIZE, BATCH_SIZE, N_LAYERS, DEVICE).to(DEVICE)\n",
    "bass_vae = music_generation_VAE.ConvVAE([\"Bass\"], VAE_HIDDEN_SIZE)\n",
    "bass_learner.load_state_dict(torch.load(BASS_LEARNER))\n",
    "bass_learner.to(DEVICE)\n",
    "bass_vae.load_state_dict(torch.load(BASS_VAE))\n",
    "bass_vae.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b54b7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "drums_learner = feature_prediction_gru.FeaturePredictionGRU([\"Drums\"], VAE_HIDDEN_SIZE+1, \n",
    "                                                             LEARNER_HIDDEN_SIZE, BATCH_SIZE, N_LAYERS, DEVICE).to(DEVICE)\n",
    "drums_vae = music_generation_VAE.ConvVAE([\"Drums\"], VAE_HIDDEN_SIZE)\n",
    "drums_learner.load_state_dict(torch.load(DRUMS_LEARNER))\n",
    "drums_learner.to(DEVICE)\n",
    "drums_vae.load_state_dict(torch.load(DRUMS_VAE))\n",
    "drums_vae.to(DEVICE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
